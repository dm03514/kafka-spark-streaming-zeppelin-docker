{
  "paragraphs": [
    {
      "text": "%sh\npip install kafka-python\ncd /zeppelin \u0026\u0026 wget https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-8-assembly_2.11/2.0.2/spark-streaming-kafka-0-8-assembly_2.11-2.0.2.jar \u0026\u0026 ls /zeppelin",
      "user": "anonymous",
      "dateUpdated": "2020-09-20 18:16:46.655",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/sh"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1556538860554_-1121550077",
      "id": "20190429-115420_1698791579",
      "dateCreated": "2019-04-29 11:54:20.554",
      "dateStarted": "2020-09-20 18:16:46.768",
      "dateFinished": "2020-09-20 18:16:50.408",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.dep\n\nz.reset()\nz.load(\"/zeppelin/spark-streaming-kafka-0-8-assembly_2.11-2.0.2.jar\")",
      "user": "anonymous",
      "dateUpdated": "2020-09-20 18:16:55.700",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "res0: org.apache.zeppelin.dep.Dependency \u003d org.apache.zeppelin.dep.Dependency@e675f29\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1600565983053_358183074",
      "id": "20200920-013943_331382546",
      "dateCreated": "2020-09-20 01:39:43.053",
      "dateStarted": "2020-09-20 18:16:55.759",
      "dateFinished": "2020-09-20 18:17:05.277",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "",
      "text": "%producer.pyspark\n\nimport random\n\ntenants \u003d list(range(5))\nusers \u003d list(range(10000))\nlogin_events \u003d [\n    \u0027WIDGET_SHOW\u0027, \n    \u0027CAPTCHA_COMPLETE\u0027,\n    \u0027FAIL\u0027,\n    \u0027COMPLETE\u0027\n]\n\ndef record(ts):\n    user_id \u003d random.choice(users)\n    return {\n        \"id\": \"evt_32132112\",\n        \"type\": random.choice(login_events),\n        \"time\": ts,\n        \"ip\": \"182.29.170.142\",\n        \"region\": \"eu\",\n        \"email\": \"{user_id}_test@test_{user_id}.com\".format(user_id\u003duser_id),\n        \"tenant_id\": random.choice(tenants)\n    }\n",
      "user": "anonymous",
      "dateUpdated": "2020-09-20 18:54:09.747",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python",
        "editorHide": false,
        "tableHide": false,
        "title": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1556120282490_1275576273",
      "id": "20190424-153802_2004623441",
      "dateCreated": "2019-04-24 15:38:02.490",
      "dateStarted": "2020-09-20 18:54:09.771",
      "dateFinished": "2020-09-20 18:54:09.822",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%producer.pyspark\nimport time\nimport json\nimport random\nimport logging\n\nfrom kafka import KafkaProducer\nfrom kafka.errors import KafkaError\n\nKAFKA_BROKER \u003d \"host.docker.internal:9092\"\nKAFKA_TOPIC \u003d \"logins\"\n\nproducer \u003d KafkaProducer(bootstrap_servers\u003d[KAFKA_BROKER])\nindex \u003d 0\n\nwhile True:\n    \n    row_dict \u003d record(time.time())\n    \n    future \u003d producer.send(\n        topic\u003dKAFKA_TOPIC, \n        key\u003dstr(row_dict[\"id\"]).encode(\"utf-8\"),\n        value\u003djson.dumps(row_dict).encode(\"utf-8\"))\n    \n    try:\n        record_metadata \u003d future.get(timeout\u003d10)\n    except KafkaError:\n        # Decide what to do if produce request failed...\n        logging.exception(\"Error\")\n        pass\n    \n    producer.flush()\n    \n    index +\u003d 1\n    time.sleep(random.uniform(0.1,3.0))",
      "user": "anonymous",
      "dateUpdated": "2020-09-20 18:58:41.626",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1556124778874_694089528",
      "id": "20190424-165258_808657351",
      "dateCreated": "2019-04-24 16:52:58.874",
      "dateStarted": "2020-09-20 18:58:41.650",
      "dateFinished": "2020-09-20 19:02:43.984",
      "status": "ABORT",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\n\nimport json\nfrom pyspark.streaming.kafka import KafkaUtils\nfrom pyspark.sql import types\nfrom pyspark.streaming import StreamingContext\nfrom kafka import KafkaProducer\nfrom kafka.errors import KafkaError\nimport os\n\nimport hashlib\n\n# os.environ[\u0027PYSPARK_SUBMIT_ARGS\u0027] \u003d \u0027--jars /zeppelin/spark-streaming-kafka-0-8-assembly_2.11-2.0.2.jar pyspark-shell\u0027 \n\n\nKAFKA_BROKER \u003d \"host.docker.internal:9092\"\nKAFKA_TOPIC \u003d \"logins_cleaned\"\n\n\ntry:\n    # Reset streaming context if exists\n    ssc.stop(stopSparkContext\u003dFalse, stopGraceFully\u003dFalse)\nexcept:\n    pass\n\nssc \u003d StreamingContext(sc, batchDuration\u003d2)\n\nLOGINS_TOPIC \u003d \"logins\"\nKAFKA_BROKERS \u003d \"host.docker.internal:9092,host.docker.internal:9092\"\n\nstream \u003d KafkaUtils.createDirectStream(\n                            ssc, \n                            [LOGINS_TOPIC], \n                            {\"metadata.broker.list\": KAFKA_BROKERS})\n\ndef publish(login_event):\n    producer \u003d KafkaProducer(bootstrap_servers\u003d[KAFKA_BROKER])\n    future \u003d producer.send(\n        topic\u003dKAFKA_TOPIC, \n        key\u003dstr(login_event[\"id\"]).encode(\"utf-8\"),\n        value\u003djson.dumps(login_event).encode(\"utf-8\"))\n    \n    try:\n        record_metadata \u003d future.get(timeout\u003d10)\n    except KafkaError:\n        # Decide what to do if produce request failed...\n        logging.exception(\"Error\")\n        pass\n    \n    producer.flush()\n    return login_event\n\n\ndef clean_login_secrets(ld):\n    m \u003d hashlib.md5()\n    m.update(ld[\u0027ip\u0027])\n    ld[\u0027ip\u0027] \u003d m.hexdigest()\n    \n    m \u003d hashlib.md5()\n    m.update(ld[\u0027email\u0027])\n    ld[\u0027email\u0027] \u003d m.hexdigest()\n    return ld\n\nstream \\\n    .map(lambda x: json.loads(x[1])) \\\n    .map(clean_login_secrets) \\\n    .map(publish) \\\n    .pprint()\n\nssc.start()\nssc.awaitTermination()",
      "user": "anonymous",
      "dateUpdated": "2020-09-20 19:02:09.568",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1556022009133_-1199138898",
      "id": "20190423-122009_873241770",
      "dateCreated": "2019-04-23 12:20:09.133",
      "dateStarted": "2020-09-20 19:02:09.607",
      "dateFinished": "2020-09-20 19:02:41.720",
      "status": "ABORT",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Example Kafka Spark Streaming",
  "id": "2EAB941ZD",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {
    "producer:shared_process": [],
    "sh:shared_process": [],
    "consumer:shared_process": [],
    "spark:shared_process": []
  },
  "config": {
    "isZeppelinNotebookCronEnable": false,
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {}
}